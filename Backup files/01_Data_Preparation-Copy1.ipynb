{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01: Quoth The Raven NLG - Data Loading and Preprocessing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, load in the text files for the complete works of Edgar Allan Poe and prepare the text for training natural language generating models.\n",
    "\n",
    "* Section A: Loading and Cleaning Text Files\n",
    "* Section B: Generating Word-Level Tokens of the Text Data\n",
    "* Section C: Generating Sequences of Tokens and Saving to a File for Modeling and Language Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from keras.preprocessing.text import text_to_word_sequence, hashing_trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"load\"></a>Section A: Loading and Cleaning the Text Files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data folder, you'll notice a few different items:\n",
    "* Original text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(path, file_encoding=None):\n",
    "    \n",
    "    # make a list of all the text files in the specified directory\n",
    "    text_files = [file for file in os.listdir(path)]\n",
    "    \n",
    "    print(f'The following {len(text_files)} have been loaded:')\n",
    "    for _ in range(len(text_files)):\n",
    "        print(text_files[_])\n",
    "    \n",
    "    # create variable to hold the text from our combined documents\n",
    "    loaded_text = ''\n",
    "    \n",
    "    # open and append the file contents to our com\n",
    "    for file in text_files:\n",
    "        loaded_text += open(path + file, encoding=file_encoding).read()\n",
    "        loaded_text += ' '\n",
    "    \n",
    "    # gets rid of extra spaces due to project gutenberg formatting\n",
    "    loaded_text = ' '.join(loaded_text.split())\n",
    "    \n",
    "    # converting to ASCII to get rid of smart quotes and some special characters\n",
    "    loaded_text = unidecode(loaded_text)\n",
    "    \n",
    "    print()\n",
    "    print(f'The length of the combined documents (in characters) is: {len(loaded_text)}')\n",
    "    \n",
    "    return loaded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following 5 have been loaded:\n",
      "CompletePoeVol3-trimmed.txt\n",
      "CompletePoeVol4-trimmed.txt\n",
      "CompletePoeVol1-trimmed.txt\n",
      "CompletePoeVol5-prose-trimmed.txt\n",
      "CompletePoeVol2-trimmed.txt\n",
      "\n",
      "The length of the combined documents (in characters) is: 2296101\n"
     ]
    }
   ],
   "source": [
    "raw_text = load_corpus('./data/Poe_NLG/02_Poe_author_text_only/Prose/', 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CompletePoeVol3-trimmed.txt', 'CompletePoeVol4-trimmed.txt', 'CompletePoeVol1-trimmed.txt', 'CompletePoeVol5-prose-trimmed.txt', 'CompletePoeVol2-trimmed.txt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2296101"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_files = [file for file in os.listdir('./data/Poe_NLG/02_Poe_author_text_only/Prose/')]\n",
    "print(text_files)\n",
    "raw_text = ''\n",
    "\n",
    "for file in text_files:\n",
    "    raw_text += open(f'./data/Poe_NLG/02_Poe_author_text_only/Prose/{file}', encoding='utf-8').read()\n",
    "    raw_text += ' '\n",
    "raw_text = ' '.join(raw_text.split())\n",
    "raw_text = unidecode(raw_text)\n",
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '\"', '$', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_v2 = re.sub('[^A-Za-z!?.,\"\\':;-]+',' ',raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_v2 = raw_text_v2.replace('--', '&')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_as_word = ['!', '?', '.', ',', '\"', \"'\", ':', ';', '&', '-']\n",
    "for punc in punctuation_as_word:\n",
    "    raw_text_v2 = raw_text_v2.replace(punc, f' {punc} ')\n",
    "\n",
    "raw_text_v2 =  raw_text_v2.replace(' & ', ' -- ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_v2 = re.sub('\\s\\s+', ' ', raw_text_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = raw_text_v2.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Upon', 'my', 'return', 'to', 'the', 'United', 'States', 'a', 'few', 'months']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399322"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43940"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences created: 480066\n"
     ]
    }
   ],
   "source": [
    "# create sequences of tokens to use with Keras Tokenizer class for use in modeling\n",
    "seq_input_len = 3\n",
    "seq_tot_len = seq_input_len + 1\n",
    "sequences = []\n",
    "for i in range(seq_tot_len, len(tokens)):\n",
    "    token_seq = tokens[i-seq_tot_len:i]\n",
    "    sequence = ' '.join(token_seq)\n",
    "    sequences.append(sequence)\n",
    "\n",
    "print(f'Total Sequences created: {len(sequences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned and prepped sequences\n",
    "def save_sequences_to_file(sequences, filename):\n",
    "    sequence_lines = '\\n'.join(sequences)\n",
    "    file = open(f'./data/Poe_NLG/03_Text_objects_for_models/{filename}', 'w')\n",
    "    file.write(sequence_lines)\n",
    "    file.close()\n",
    "\n",
    "save_sequences_to_file(sequences, f'cleaned_poe_tot_seq_len_{seq_tot_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-2954214b8eab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
