{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01: Quoth The Raven NLG - Data Loading and Preprocessing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, load in the text files for the complete works of Edgar Allan Poe and prepare the text for training natural language generating models.\n",
    "\n",
    "* [Section A: Loading Corpus](#load)\n",
    "* [Section B: Cleaning the Loaded Text](#clean)\n",
    "* [Section C: Generating Word-Level Tokens of the Text Data and Sequences of Tokens](#token)\n",
    "* [Section D: Saving the Sequences to a File for Use in Modeling](#save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "from keras.preprocessing.text import text_to_word_sequence, hashing_trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### <a name=\"load\"></a>Section A: Loading and Cleaning the Text Files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data folder, you'll notice a few different items:\n",
    "* Original text files from Project Gutenberg\n",
    "    * These are the original full version with no editing or cleaning.\n",
    "* Trimmed version of the files separated into `Prose` and `Poetry` folders \n",
    "    * Since the ultimate goal of this project is to generate micro-stories about an image, only Poe's prose is currently being used to train the model.\n",
    "    * Non-Poe text has been manually removed from as much of this text as possible including the Project Gutenberg headers and footers, editor notes, as well as chapters and titles to focus as much on Poe's complete language as possible.\n",
    "\n",
    "*Aside:* My original desire was to utilize web scraping to acquire the texts, but many of the sites were either incomplete or forbade scraping. With the data easily and ethically accessible via Project Gutenberg, I decided to use the text files available there.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the data from the trimmed data files\n",
    "def load_corpus(path, file_encoding=None):\n",
    "    \n",
    "    # make a list of all the text files in the specified directory\n",
    "    text_files = [file for file in os.listdir(path)]\n",
    "    \n",
    "    print(f'The following {len(text_files)} file(s) have been loaded:')\n",
    "    for _ in range(len(text_files)):\n",
    "        print(text_files[_])\n",
    "    \n",
    "    # create variable to hold the text from our combined documents\n",
    "    loaded_text = ''\n",
    "    \n",
    "    # open and append the file contents to our com\n",
    "    for file in text_files:\n",
    "        loaded_text += open(path + file, encoding=file_encoding).read()\n",
    "        loaded_text += ' '\n",
    "    \n",
    "    # gets rid of extra spaces due to project gutenberg formatting\n",
    "    loaded_text = ' '.join(loaded_text.split())\n",
    "    \n",
    "    # converting to ASCII to get rid of smart quotes and some special characters\n",
    "    loaded_text = unidecode(loaded_text)\n",
    "    \n",
    "    print()\n",
    "    print(f'The length of the combined documents (in characters) is: {len(loaded_text)}')\n",
    "    \n",
    "    return loaded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following 5 file(s) have been loaded:\n",
      "CompletePoeVol3-trimmed.txt\n",
      "CompletePoeVol4-trimmed.txt\n",
      "CompletePoeVol1-trimmed.txt\n",
      "CompletePoeVol5-prose-trimmed.txt\n",
      "CompletePoeVol2-trimmed.txt\n",
      "\n",
      "The length of the combined documents (in characters) is: 2296101\n"
     ]
    }
   ],
   "source": [
    "raw_text = load_corpus('./data/Poe_NLG/02_Poe_author_text_only/Prose/', 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### <a name=\"clean\"></a>Section B: Cleaning the Loaded Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Function to clean and prepare the loaded text for tokenization -- \n",
    "    we'll keep selected punctuation as words to see if the model can learn to apply them\n",
    "    correctly. We'll also keep captilization of words to see how the model places them.'''\n",
    "def clean_corpus(corpus, punc_to_keep):\n",
    "    \n",
    "    # creating a string of the punctuation marks to keep \n",
    "    punc_string = ''.join([c if c != \"'\" else \"\\'\" for c in punc_to_keep])\n",
    "    \n",
    "    ''' Using regex to get rid of all numberic and special characters.\n",
    "        Also swapping out the ASCII double dash with '&' since we want to keep the them separate\n",
    "        and don't want them treated as two individual dashes.''' \n",
    "    corpus = re.sub('[^A-Za-z'+punc_string+']+',' ', corpus)\n",
    "   \n",
    "    corpus = corpus.replace('--', ' & ')\n",
    "    \n",
    "    # putting spacing around punctuation so they are treated as their own words during tokenization\n",
    "    for punc in punc_to_keep:\n",
    "        corpus = corpus.replace(punc, f' {punc} ')\n",
    "    \n",
    "    # putting our double dashes (em-dashes) back into the corpus\n",
    "    corpus = corpus.replace(' & ', ' -- ')\n",
    "    \n",
    "    # removing any additional spaces\n",
    "    corpus = re.sub('\\s\\s+', ' ', corpus)\n",
    "    \n",
    "    # returning our cleaned data\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "punc_to_keep = ['!', '?', '.', ',', '\"', \"'\", ':', ';', '-'] # cannot include '&' for use in function\n",
    "cleaned_text = clean_corpus(raw_text, punc_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <a name=\"token\">Section C: Generating Word-Level Tokens of the Text Data and Sequences of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to separate text into word tokens and put them into sequences\n",
    "def create_tokens_and_sequences(cleaned_text, sequence_input_length):\n",
    "    \n",
    "    # creating tokens by splitting text at spaces\n",
    "    tokens = cleaned_text.split()\n",
    "    print(f'Total tokens created from text: {len(tokens)}')\n",
    "    print(f'Unique tokens created from text: {len(set(tokens))}')\n",
    "    \n",
    "    # total length of generated sequences will be the designated number of words + the next word to be predicted\n",
    "    seq_total_len = sequence_input_length + 1\n",
    "    \n",
    "    # variable to hold our generated sequences\n",
    "    sequences = []\n",
    "    for i in range (seq_total_len, len(tokens)):\n",
    "        selected_tokens = tokens[i - seq_total_len: i]\n",
    "        sequence = ' '.join(selected_tokens)\n",
    "        sequences.append(sequence)\n",
    "        \n",
    "    print(f'{len(sequences)} sequences were created.')\n",
    "    print(f'Each sequence is {seq_total_len} word(s) in length with {sequence_input_length} word(s) preceding one output word.')\n",
    "    \n",
    "    return tokens, sequences, seq_total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens created from text: 480070\n",
      "Unique tokens created from text: 22467\n",
      "479969 sequences were created.\n",
      "Each sequence is 101 word(s) in length with 100 word(s) preceding one output word.\n"
     ]
    }
   ],
   "source": [
    "tokens, sequences, seq_total_len = create_tokens_and_sequences(cleaned_text, sequence_input_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Upon', 'my', 'return', 'to', 'the', 'United', 'States', 'a', 'few', 'months']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Upon my return to the United States a few months ago , after the extraordinary series of adventure in the South Seas and elsewhere , of which an account is given in the following pages , accident threw me into the society of several gentlemen in Richmond , Va . , who felt deep interest in all matters relating to the regions I had visited , and who were constantly urging it upon me , as a duty , to give my narrative to the public . I had several reasons , however , for declining to do so , some',\n",
       " 'my return to the United States a few months ago , after the extraordinary series of adventure in the South Seas and elsewhere , of which an account is given in the following pages , accident threw me into the society of several gentlemen in Richmond , Va . , who felt deep interest in all matters relating to the regions I had visited , and who were constantly urging it upon me , as a duty , to give my narrative to the public . I had several reasons , however , for declining to do so , some of',\n",
       " 'return to the United States a few months ago , after the extraordinary series of adventure in the South Seas and elsewhere , of which an account is given in the following pages , accident threw me into the society of several gentlemen in Richmond , Va . , who felt deep interest in all matters relating to the regions I had visited , and who were constantly urging it upon me , as a duty , to give my narrative to the public . I had several reasons , however , for declining to do so , some of which',\n",
       " 'to the United States a few months ago , after the extraordinary series of adventure in the South Seas and elsewhere , of which an account is given in the following pages , accident threw me into the society of several gentlemen in Richmond , Va . , who felt deep interest in all matters relating to the regions I had visited , and who were constantly urging it upon me , as a duty , to give my narrative to the public . I had several reasons , however , for declining to do so , some of which were',\n",
       " 'the United States a few months ago , after the extraordinary series of adventure in the South Seas and elsewhere , of which an account is given in the following pages , accident threw me into the society of several gentlemen in Richmond , Va . , who felt deep interest in all matters relating to the regions I had visited , and who were constantly urging it upon me , as a duty , to give my narrative to the public . I had several reasons , however , for declining to do so , some of which were of']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <a name=\"save\">Section D: Saving the Sequences to a File for Use in Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned and prepped sequences\n",
    "def save_sequences_to_file(sequences, filename):\n",
    "    sequence_lines = '\\n'.join(sequences)\n",
    "    file = open(f'./data/Poe_NLG/03_Text_files_for_models/{filename}', 'w')\n",
    "    file.write(sequence_lines)\n",
    "    file.close()\n",
    "\n",
    "save_sequences_to_file(sequences, f'cleaned_poe_tot_seq_len_{seq_total_len}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi] *",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
